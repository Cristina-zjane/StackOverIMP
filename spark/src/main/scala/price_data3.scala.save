import org.apache.spark.SparkContext

import org.apache.spark.SparkContext._

import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.SQLContext._

import org.apache.spark.SparkConf

//import org.apache.spark.sql._

import org.elasticsearch.spark._
import org.elasticsearch.spark.sql._
//import redis.clients.jedis.Jedis

object price_data3 {

 def main(args: Array[String]) {

   case class TagsStat(tags: String, ave_time: Long, no_aws:Int,no_no_aws:Int)

   // setup the Spark Context named sc
   val format = new java.text.SimpleDateFormat("yyyy-MM-dd kk:mm:ss")
   val conf = new SparkConf().setAppName("PriceDataExercise")
   //conf.set("es.resource", "tags");
  
   conf.set("es.index.auto.create", "true")
   conf.set("es.nodes", "localhost,172.31.0.68")
   conf.set("es.port", "9200")
   val sc = new SparkContext(conf)
   //import sqlContext.implicits._
   val sqlContext = new SQLContext(sc)
   if(args(0)=="1")//initial content
   { 

   val lines=sc.textFile("hdfs://ec2-52-40-160-114.us-west-2.compute.amazonaws.com:9000/user/data/"+args(1))
   val answers=lines.filter(l=>l.split(",")(1)=="1").map(l=>(l.split(",")(2),l)).reduceByKey((a,b)=>if(format.parse(a.split(",")(3)).getTime()<format.parse(b.split(",")(3)).getTime())a else b )
   val questions=lines.filter(l=>l.split(",")(1)=="0").map(l=>(l.split(",")(0),l))
   val res_q=questions.leftOuterJoin(answers)
   .map(
   // a=>a._2
    a=>(rangeTags(a._2._1.split(",")(4),";"),(a._2._2 match {

        case Some(s)=> Math.abs(format.parse(a._2._1.split(",")(3)).getTime()-format.parse(s.split(",")(3)).getTime())
        case None => 0
    },
     a._2._2 match {
        case Some(s) => 1
        case None =>0
     },
     a._2._2 match {
        case Some(s) => 0
        case None =>1
     },
     a._2._1
    ))
   )

  val no_aws_qs=res_q.filter(a=>a._2._1==0).map(a=>a._2._4).saveAsTextFile("hdfs://ec2-52-40-160-114.us-west-2.compute.amazonaws.com:9000/user/data/"+args(1)+"_no")

  val res_f=res_q.reduceByKey((a,b)=>(a._1+b._1,a._2+b._2,a._3+b._3,""))
      .map(
        a=>TagsStat(a._1,(a._2._2 match {case 0=> 0 case _ => a._2._1/a._2._2 }).toLong,a._2._2,a._2._3)
        )

  //"ave_time"-> (a._2._2 match {case 0=> 0 case _ => a._2._1/a._2._2 }),
                //"no_aws"->a._2._2,
                //"no_no_aws"->a._2._3


   println("---------------------+++++++++++++++++++++++++")
   //res_f.saveToEs("ela/test5"
   //,Map("es.mapping.id" -> "tags")
   //)
   //val numbers = Map("one" -> 1, "two" -> 2, "three" -> 3)
   //val airports = Map("arrival" -> "Otopeni", "SFO" -> "San Fran")
   //sc.makeRDD(Seq(numbers, airports)).collect.foreach(a=>(a))
   //sc.makeRDD(Seq(numbers, airports)).saveToEs("spark/docs")
   res_f.collect.foreach(a=>println(a))
   println(questions.count)
   }
   else{
      //arg[1]is file name,should be associated with nth day
      //val lines:RDD[String]
      val line1=sc.textFile("hdfs://ec2-52-40-160-114.us-west-2.compute.amazonaws.com:9000/user/data/"+args(1))
      //no left content
      //if(args(2)=="0"){
       // val lines=line1
      //}
      //else{// merge left content
        val line2=sc.textFile("hdfs://ec2-52-40-160-114.us-west-2.compute.amazonaws.com:9000/user/data/"+args(2))
        val lines = line1.union(line2)
      //}
      
      val answers=lines.filter(l=>l.split(",")(1)=="1").map(l=>(l.split(",")(2),l)).reduceByKey((a,b)=>if(format.parse(a.split(",")(3)).getTime()<format.parse(b.split(",")(3)).getTime())a else b )
      
      val questions=lines.filter(l=>l.split(",")(1)=="0").map(l=>(l.split(",")(0),l))
     val res_q=questions.leftOuterJoin(answers)
               .map(
               // a=>a._2
                a=>(rangeTags(a._2._1.split(",")(4),";"),(a._2._2 match {

                    case Some(s)=> Math.abs(format.parse(a._2._1.split(",")(3)).getTime()-format.parse(s.split(",")(3)).getTime())
                    case None => 0
                },
                 a._2._2 match {
                    case Some(s) => 1
                    case None =>0
                 },
                 a._2._2 match {
                    case Some(s) => 0
                    case None =>1
                 },
                 a._2._1
                ))
               )


     val no_aws_qs=res_q.filter(a=>a._2._1==0).map(a=>a._2._4)
     no_aws_qs.saveAsTextFile("hdfs://ec2-52-40-160-114.us-west-2.compute.amazonaws.com:9000/user/data/"+args(1)+"_no")
     val period=args(3).toLong   //periodic time of doing the job in milisecond
     val compareTime=10*60*1000 //10 mins
     val latest_no_aws_qs=no_aws_qs.filter(a=>(Math.abs(format.parse(a.split(",")(3)).getTime()-format.parse(args(1)).getTime()) < period )
	//&& 
	//(format.parse(a.split(",")(3)).getTime()+ compareTime< format.parse(args(1)).getTime())
	)
     val num_no_aws_qs_tags=latest_no_aws_qs.map(a=>(rangeTags(a.split(",")(4),";"),1)).reduceByKey((a,b)=>a+b)
     //....update database
      
     //....update database
    
     
     val aws_qs_in_time_bef_red=res_q.filter(a=>a._2._1!=0 && a._2._1 <= compareTime)
     val aws_qs_in_time= aws_qs_in_time_bef_red.reduceByKey((a,b)=>(a._1+b._1,a._2+b._2,0,"")).map(a=>(a._1,("in time:",a._2._2)))
     val aws_qs_no_in_time=res_q.filter(a=>a._2._1!=0 && a._2._1 > compareTime).reduceByKey((a,b)=>(a._1+b._1,a._2+b._2,0,"")).map(a=>(a._1,a._2._2))

     val aws_qs_no_in_time_total=aws_qs_no_in_time.fullOuterJoin(num_no_aws_qs_tags).reduceByKey((a,b)=>a+b).map(a=>(a._1,("over time:",a._2)))
     aws_qs_in_time.fullOuterJoin(aws_qs_no_in_time_total).toDF().toJSON.saveAsTextFile("hdfs://ec2-52-40-160-114.us-west-2.compute.amazonaws.com:9000/user/data/tmp/"+args(1)+"_tmp")



     
    //....updata database

    //....update database
    


    
     

     //questions being answered and not being answered in this time slot
     //val qNoAws=res_q.filter._


   }  
 }
 def rangeTags(tags : String, splitter: String) : String ={
  val sortedTags=tags.split(splitter).sorted.mkString(splitter)
  return sortedTags

 }

}
